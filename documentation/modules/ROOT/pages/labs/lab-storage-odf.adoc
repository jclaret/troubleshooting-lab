= Storage - OpenShift Data Foundation Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

== Lab Overview

This comprehensive lab covers troubleshooting OpenShift Data Foundation (ODF) across multiple scenarios including Ceph cluster health issues, capacity management, file storage problems, block storage misconfigurations, and S3 object storage connectivity issues. You'll learn to diagnose and resolve storage problems in OpenShift environments.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand ODF architecture and Ceph cluster components
* Use ODF debugging tools including Ceph toolbox and must-gather
* Diagnose and resolve Ceph cluster health issues
* Troubleshoot storage capacity alerts and read-only conditions
* Resolve file storage (CephFS) capacity and mounting issues
* Fix block storage (RBD) multi-attach and access mode problems
* Debug S3 object storage connectivity and authentication issues
* Apply systematic troubleshooting methodology for storage problems

=== Reference Documentation

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[Troubleshooting OpenShift Data Foundation]
* link:https://docs.ceph.com/en/quincy/rados/troubleshooting/[Ceph Troubleshooting Guide]

== OpenShift Data Foundation Architecture Overview

OpenShift Data Foundation provides software-defined storage using Ceph as the underlying storage system. Key components include:

* **Ceph Monitors (MONs)**: Maintain cluster maps and coordinate between components
* **Object Storage Daemons (OSDs)**: Store data and handle replication/recovery
* **Managers (MGRs)**: Track metrics and expose cluster information via REST API
* **Metadata Servers (MDSes)**: Store metadata for CephFS file system operations
* **RADOS Gateway (RGW)**: Provides S3-compatible object storage interface

=== Understanding Storage Types and Access Modes

ODF provides different storage types through dynamic provisioning. Understanding these options is crucial for troubleshooting storage issues:

**Storage Classes and Access Modes:**

[cols="2,2,2,2,3",options="header"]
|===
|Access Mode |Backend/CSI Driver |Volume Mode |Filesystem |Storage Class Notes

|RWO, RWX
|Ceph RBD
|Block
|Raw block device
|RBD StorageClass with `openshift-storage.rbd.csi.ceph.com` provisioner

|RWO
|Ceph RBD
|Filesystem
|`ext4` (default)
|RBD StorageClass with `openshift-storage.rbd.csi.ceph.com` provisioner

|RWO, RWX
|CephFS
|Filesystem
|CephFS managed filesystem
|CephFS StorageClass with `openshift-storage.cephfs.csi.ceph.com` provisioner

|RWOP
|CephFS/RBD/NFS
|Filesystem or Block
|Depends on backend
|ReadWriteOncePod mode for single-pod access
|===

**Key Points for Troubleshooting:**
- **RWO (ReadWriteOnce)**: Can only be mounted by one pod at a time
- **RWX (ReadWriteMany)**: Can be mounted by multiple pods simultaneously  
- **RWOP (ReadWriteOncePod)**: Ensures only one pod can access the volume in the entire cluster
- **Block vs Filesystem**: Block volumes are raw devices, filesystem volumes are pre-formatted

=== Information Gathering: ODF Component Status

Check the overall status of ODF components:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf

# View storage cluster status
oc get storagecluster -n openshift-storage

# Check Ceph cluster health
oc get cephcluster -n openshift-storage

# View all ODF pods
oc get pods -n openshift-storage

# Check ODF dashboard status
oc get route -n openshift-storage
----

== ODF Troubleshooting Commands

When troubleshooting OpenShift Data Foundation issues, use these commands to investigate and resolve problems:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf
oc get storagecluster -n openshift-storage
oc get cephcluster -n openshift-storage

# View all ODF pods and their status
oc get pods -n openshift-storage -o wide

# Check ODF dashboard and routes
oc get route -n openshift-storage

# Check Ceph cluster health using toolbox
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph health detail

# Check OSD tree and status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd tree
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd stat

# View cluster capacity and usage
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph df

# Check storage classes and PVCs
oc get storageclass | grep ocs
oc get pvc -A | grep ocs

# Check node labels for ODF storage nodes
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"

# Check for ODF-related alerts
oc get alerts -n openshift-storage
oc get events -n openshift-storage --sort-by='.lastTimestamp' | grep -i "ceph\|odf\|storage"

# Check Ceph monitor status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon stat
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon dump

# Check CephFS status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph fs status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mds stat

# Check RGW (S3) status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph rgw stat
oc get pods -n openshift-storage | grep rgw

# Generating logs for Ceph or OpenShift Data Foundation cluster
oc logs rook-ceph-operator-<ID> -n openshift-storage

# Generating logs for plugin pods like cephfs or rbd to detect any problem in the PVC mount of the app-pod
oc logs csi-cephfsplugin-<ID> -n openshift-storage -c csi-cephfsplugin
oc logs csi-rbdplugin-<ID> -n openshift-storage -c csi-rbdplugin

# To generate logs for all the containers in the CSI pod
oc logs csi-cephfsplugin-<ID> -n openshift-storage --all-containers
oc logs csi-rbdplugin-<ID> -n openshift-storage --all-containers

# Generating logs for cephfs or rbd provisioner pods to detect problems if PVC is not in BOUND state
oc logs csi-cephfsplugin-provisioner-<ID> -n openshift-storage -c csi-cephfsplugin
oc logs csi-rbdplugin-provisioner-<ID> -n openshift-storage -c csi-rbdplugin

# To generate logs for all the containers in the CSI pod:
oc logs csi-cephfsplugin-provisioner-<ID> -n openshift-storage --all-containers
oc logs csi-rbdplugin-provisioner-<ID> -n openshift-storage --all-containers

# Generating OpenShift Data Foundation logs using cluster-info command
oc cluster-info dump -n openshift-storage --output-directory=<directory-name>

# When using Local Storage Operator, generating logs can be done using cluster-info command:
oc cluster-info dump -n openshift-local-storage --output-directory=<directory-name>

# To check the operator logs
oc logs -n openshift-storage deployment/ocs-operator --tail=50

# To check the operator events
oc get events --sort-by=metadata.creationTimestamp -n openshift-storage

# Get the OpenShift Data Foundation operator version and channel
oc get csv -n openshift-storage
oc get subs -n openshift-storage
oc get installplan -n openshift-storage

# Monitor ODF operator logs
oc logs -n openshift-storage deployment/ocs-operator --tail=50

# Check Ceph cluster status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status

# Verify ODF components
oc get pods -n openshift-storage
----

== Collecting Diagnostic Information with Must-Gather 

If Red Hat OpenShift Data Foundation is unable to automatically resolve a problem, use the must-gather tool to collect log files and diagnostic information so that you or Red Hat support can review the problem and determine a solution.

Run the must-gather command from the client connected to the OpenShift Data Foundation cluster:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc adm must-gather --image=registry.redhat.io/odf4/odf-must-gather-rhel9:v4.18 --dest-dir=<directory-name>
----

<directory-name> is the name of the directory where you want to write the data to.

This collects the following information in the specified directory:

- All Red Hat OpenShift Data Foundation cluster related Custom Resources (CRs) with their namespaces.
- Pod logs of all the Red Hat OpenShift Data Foundation related pods.
- Output of some standard Ceph commands like Status, Cluster health, and others.

To specify a relative time period for logs gathered, such as within 5 seconds or 2 days

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
oc adm must-gather --image=$(oc -n openshift-storage get deployment.apps/ocs-operator -o jsonpath='{.spec.template.metadata.annotations.operators\.openshift\.io/must-gather-image}') --dest-dir=_<directory-name>_ /usr/bin/gather since=5h
oc adm must-gather --image=$(oc -n openshift-storage get deployment.apps/ocs-operator -o jsonpath='{.spec.template.metadata.annotations.operators\.openshift\.io/must-gather-image}') --dest-dir=_<directory-name>_ /usr/bin/gather since-time=2020-11-10T04:00:00+00:00 (starting from 4 am UTC on 11 Nov 2020)
----

=== Running must-gather in modular mode 

OpenShift Data Foundation must-gather can take a long time to run in some environments. To avoid this, run must-gather in modular mode and collect only the resources you require using the following command:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# ODF logs (includes Ceph resources, namespaced resources, clusterscoped resources and Ceph logs)
oc adm must-gather --image=registry.redhat.io/odf4/odf-must-gather-rhel9:v4.18 -- /usr/bin/gather --odf

# Noobaa logs
oc adm must-gather --image=registry.redhat.io/odf4/odf-must-gather-rhel9:v4.18 -- /usr/bin/gather --noobaa

# Ceph commands and pod logs
oc adm must-gather --image=registry.redhat.io/odf4/odf-must-gather-rhel9:v4.18 -- /usr/bin/gather --ceph

# Ceph daemon, kernel, journal logs, and crash reports
oc adm must-gather --image=registry.redhat.io/odf4/odf-must-gather-rhel9:v4.18 -- /usr/bin/gather --ceph-logs
----

== Troubleshooting ODF Health and Alerts

OpenShift Data Foundation continuously monitors cluster health and can automatically resolve many common issues. However, some problems require manual intervention and troubleshooting.

**Where to Check for Issues:**

The OpenShift Web Console provides several locations to monitor ODF health:

* **Observe → Alerting → Firing**: Shows active alerts that need attention
* **Home → Overview → Cluster**: Displays overall cluster health status
* **Storage → Data Foundation → Storage System**: Detailed storage system information
* **Storage → Data Foundation → Storage System → Overview**: Block, File, and Object storage status

**Understanding Health Checks:**

ODF uses a comprehensive set of health checks to monitor cluster status. Each health check has a unique identifier and provides specific information about potential issues. These checks help administrators quickly identify and resolve storage problems before they impact applications. 

=== Common ODF Issues and Troubleshooting Approaches

**Reference Documentation:**
For a complete list of health alerts and their resolutions, see the link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/troubleshooting_openshift_data_foundation/index#resolving-cluster-alerts_rhodf[ODF Troubleshooting Guide].

**Common Issue Categories:**

=== CephClusterErrorState
**What it means:** The storage cluster has been in an ERROR state for more than 10 minutes, indicating a critical storage availability issue.

**Impact:** Critical - Applications may not be able to access storage.

**Troubleshooting Steps:**
1. Check for other alerts that may have triggered this one
2. Examine pod status and resource availability
3. Verify node health and kubelet status
4. Use Ceph tools to check storage component status

=== CephMdsMissingReplicas
**What it means:** The minimum required replicas for the Metadata Server (MDS) are not available, affecting CephFS storage class functionality.

**Impact:** High - CephFS volumes may not be accessible.

**Troubleshooting Steps:**
1. Check MDS pod status and resource constraints
2. Verify node assignment and resource availability
3. Examine pod logs for specific error messages
4. Ensure proper resource allocation for MDS pods

=== CephNodeDown
**What it means:** A storage node running Ceph pods has gone down, potentially affecting storage operations.

**Impact:** Medium - Storage continues to function but with reduced redundancy.

**Troubleshooting Steps:**
1. Verify node status and identify the failed node
2. Check OSD pod scheduling and recovery
3. Monitor Ceph cluster recovery process
4. Ensure resource requirements are met for new node placement

=== PersistentVolumeUsageCritical
**What it means:** A Persistent Volume Claim has exceeded 85% of its capacity, risking data loss.

**Impact:** High - Potential data loss if not addressed.

**Resolution:**
1. Expand the PVC size through the OpenShift Web Console
2. Or delete unnecessary data to free up space
3. Monitor usage patterns to prevent future issues

== Exercise 1: ODF CLI Installation and Health Check

This exercise will guide you through installing the ODF CLI tool and performing a basic health check on your ODF cluster. This is a straightforward installation and verification exercise.

=== Objective

Install the ODF CLI tool and use it to verify the health status of your OpenShift Data Foundation cluster.

=== Step 1: Install ODF CLI

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Navigate to the ODF files directory
cd /home/demo-user/openshift-troubleshooting-lab/troubleshooting-lab/lab-materials/04-storage-odf/files

# Extract the ODF CLI tool
tar xzvf odf-cli-4.18.5-linux-amd64.tar.gz

# Install ODF CLI to system PATH
sudo mv odf-amd64 /usr/local/bin/odf

# Verify installation
odf -h
----

**Expected Result:** You should see the ODF CLI help menu with available commands.

=== Step 2: Perform Health Check

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Run comprehensive health check
odf get health
----

**Expected Result:** The health check should show:
- All ODF components in healthy state
- No error messages
- Proper resource counts

=== Step 3: Verify ODF Components

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF storage cluster
odf get storagecluster -n openshift-storage

# Check Ceph cluster status
odf get cephcluster -n openshift-storage

# Check ODF operator status
odf get csv -n openshift-storage
----

**Expected Result:** All components should show healthy status with no errors.

=== Verification

Confirm that:
- ODF CLI is installed and working
- Health check shows all components healthy
- No error messages in any output
- All ODF pods are running properly

== Exercise 2: Troubleshooting ODF Health Issues

=== Scenario Setup: Storage Node Configuration Problems

Navigate to the ODF troubleshooting lab and deploy the exercise scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the ODF troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/00-odf-health

# Execute the ODF configuration script
chmod +x odf-config.sh
./odf-config.sh

# Monitor ODF cluster status
oc get storagecluster -n openshift-storage -w
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the ODF cluster behavior and identify any issues.

**What to check:**

- What is the status of the ODF storage cluster?
- Are there any alerts or warnings in the ODF console?
- Which nodes are labeled for ODF storage?
- Are there any OSD pods failing or in error state?
- What does the Ceph cluster health show?

Use the troubleshooting commands provided earlier to investigate the cluster behavior systematically.

=== Understanding the Problem

If you discovered that the ODF storage cluster is degraded and OSD pods are failing, this indicates a node labeling issue that prevents ODF from identifying storage nodes correctly.

=== Creating the Solution

**YOUR MISSION**: Identify and resolve the node labeling issue to restore ODF functionality.

**Steps to complete:**

1. **Identify the root cause** of the ODF cluster degradation
2. **Locate the affected storage nodes**
3. **Restore the missing node labels**
4. **Verify ODF cluster recovery**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation:**

Execute the fix script to restore ODF node labeling:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
./solution/odf-fix.sh
----
====

=== Verification

Monitor the ODF recovery process:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Watch ODF storage cluster status during recovery
oc get storagecluster -n openshift-storage -w

# Monitor OSD pod restart
oc get pods -n openshift-storage -w

# Check when all OSD pods are running
oc get pods -n openshift-storage | grep osd

# Verify Ceph cluster health after recovery
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status

# Verify node labels are restored
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"
----

== Exercise 3: File Storage Troubleshooting

Navigate to the file storage troubleshooting exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/02-odf-file

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-file
oc get pvc -n odf-file

# Verify PVC binding and size
oc get pvc -n odf-file
oc describe pvc upload-data -n odf-file

# Check storage class
oc describe storageclass ocs-storagecluster-cephfs

# Test application functionality
oc get route image-uploader -n odf-file -o jsonpath='{.spec.host}'
----

**Expected Application Behavior:**

- The application is expected to function on the previous HTTP route with 3 pods in Running state.
- Once all pods are in Running state, you should be able to upload all the files found in the directory `/home/demo-user/troubleshooting-lab/Photos` through the web interface. If you encounter any errors during the upload process, check the pod logs to understand what's happening.
- Application namespace is odf-file

=== Investigation Challenge

**YOUR MISSION**: Investigate the file storage issues and identify the root causes preventing the application from working properly.

**What to check:**

- How many pods are actually running and why?
- What is the current size of the PVC?
- What is the current AccessModes of the PVC?
- What happens when you try to upload files through the web interface?
- What type of storage is being used and can it be expanded?
- Can you change the AccessMode of an existing PVC?
- How could you resolve both the size and access mode issues?

**Investigation Steps:**

1. **Check pod status**: Are all 3 pods running or are some stuck in ContainerCreating/Pending?
2. **Examine PVC configuration**: Look at the current size and access modes
3. **Test file upload**: Try uploading a photo through the web interface
4. **Check pod logs**: Look for specific error messages when uploads fail
5. **Investigate storage class**: Understand what type of storage is being used

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Recreating PVC with Correct Configuration**

Execute the fix script to recreate the PVC with proper size and access mode:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-pvc.sh
./solution/fix-pvc.sh
----

**During the initial deployment, you will encounter 2 problems:**

- **AccessMode Issue**: This will cause errors and leave pods in ContainerCreating or Pending state

   ```
   Warning  FailedAttachVolume  2m51s  attachdetach-controller  Multi-Attach error for volume "pvc-d1f6cf6e-ee78-44a8-92fd-49f361113b73" Volume is already used by pod(s) image-uploader-745bb8fb7b-cb872, image-uploader-745bb8fb7b-ftmp4
   ```

- **PVC Size Issue**: This will prevent file uploads with errors like:

   ```
   OSError: [Errno 122] Disk quota exceeded
   ```

**Expected Behavior After Fix:**
After resolving both issues, the application should successfully upload all photos from the `/home/demo-user/troubleshooting-lab/Photos` directory through the web interface, with all 3 pods running and sharing the same storage volume.


====

== Exercise 4: Block Storage Troubleshooting

Navigate to the storage access exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/03-odf-block

# Deploy application with intentional issue
oc apply -k .
----

=== Information Gathering: Storage Access Investigation

Analyze the storage deployment problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check pod status
oc get pods -l app=rbd-hello -n odf-block -o wide

# Examine PVC configuration
oc get pvc -n odf-block
oc describe pvc rbd-hello-pvc -n odf-block

# Check storage class information
oc get storageclass
oc describe storageclass ocs-storagecluster-ceph-rbd

# Investigate pod scheduling issues
oc describe pod -l app=rbd-hello -n odf-block | grep -A 10 Events
----

=== Investigation Challenge

Investigate the storage access mode issue:

1. **Why can't multiple pods attach to the same volume?**
2. **What storage class is being used and what are its capabilities?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Access Mode Issues**

Execute the fix script to resolve the access mode problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-replicas.sh
./solution/fix-replicas.sh
----
====

**Expected Application Behavior:**
The application should display a "Hello World" message accessible via HTTP route once properly configured.

**Problem Analysis:**
Initially, the deployment will have 3 pods trying to use a `ocs-storagecluster-ceph-rbd` storage class with ReadWriteOnce (RWO) access mode. This creates a multi-attach error because RWO volumes can only be attached to one pod at a time.

**Error Details:**
You will see pods stuck in different states due to the multi-attach issue:

```
oc get pod
NAME                         READY   STATUS     RESTARTS   AGE
rbd-hello-74f4cbcd4c-76p6p   0/1     Init:0/1   0          2m3s
rbd-hello-74f4cbcd4c-hbhwn   0/1     Init:0/1   0          2m3s
rbd-hello-74f4cbcd4c-t6js6   0/1     Running    0          2m3s
```

**Error Message:**
```
Warning  FailedAttachVolume  2m20s  attachdetach-controller  Multi-Attach error for volume "pvc-4bffb8ac-1ab2-4b97-b1fd-4fcadbd2a64d" Volume is already used by pod(s) rbd-hello-74f4cbcd4c-t6js6
```

**Root Cause:**
The application is configured with 3 replicas but uses a ReadWriteOnce (RWO) Persistent Volume Claim, which can only be attached to one pod at a time. This causes the multi-attach error and prevents multiple pods from running simultaneously.

**Solution:**
To resolve this issue, the deployment should be scaled down to 1 replica since RWO volumes cannot be shared across multiple pods. After scaling down, the application will successfully display the "Hello World" message via the HTTP route.

== Lab Summary

This comprehensive ODF troubleshooting lab covered critical storage scenarios:

* **Storage Health**: Diagnosed and resolved node labeling issues affecting cluster performance
* **File Storage**: Resolved capacity issues through PVC expansion and storage type investigation
* **Block Storage**: Fixed multi-attach problems by understanding access modes and replica scaling
* **Object Storage**: Debugged authentication and configuration mismatches in bucket provisioning

== Additional Resources

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18[OpenShift Data Foundation Documentation]
* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html-single/troubleshooting_openshift_data_foundation/index#troubleshooting-alerts-and-errors-in-openshift-data-foundation[Troubleshooting alerts and errors in OpenShift Data Foundation]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[ODF Troubleshooting Guide]
* link:https://access.redhat.com/solutions/4502501[Red Hat Solutions for ODF Issues]
* link:https://access.redhat.com/downloads/content/547/ver=4/rhel---9/4.17.0/x86_64/product-software[Download odf-cli tool]
