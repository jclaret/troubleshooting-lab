= Storage - OpenShift Data Foundation Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

== Lab Overview

This comprehensive lab covers troubleshooting OpenShift Data Foundation (ODF) across multiple scenarios including Ceph cluster health issues, capacity management, file storage problems, block storage misconfigurations, and S3 object storage connectivity issues. You'll learn to diagnose and resolve storage problems in OpenShift environments.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand ODF architecture and Ceph cluster components
* Use ODF debugging tools including Ceph toolbox and must-gather
* Diagnose and resolve Ceph cluster health issues
* Troubleshoot storage capacity alerts and read-only conditions
* Resolve file storage (CephFS) capacity and mounting issues
* Fix block storage (RBD) multi-attach and access mode problems
* Debug S3 object storage connectivity and authentication issues
* Apply systematic troubleshooting methodology for storage problems

=== Reference Documentation

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[Troubleshooting OpenShift Data Foundation]
* link:https://docs.ceph.com/en/quincy/rados/troubleshooting/[Ceph Troubleshooting Guide]
* link:https://role.rhu.redhat.com/rol-rhu/app/seminar/exps275-1[OpenShift Data Foundation Expert Session]

== ODF Troubleshooting Commands

When troubleshooting OpenShift Data Foundation issues, use these commands to investigate and resolve problems:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf
oc get storagecluster -n openshift-storage
oc get cephcluster -n openshift-storage

# View all ODF pods and their status
oc get pods -n openshift-storage -o wide

# Check ODF dashboard and routes
oc get route -n openshift-storage

# Check Ceph cluster health using toolbox
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph health detail

# Check OSD tree and status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd tree
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd stat

# View cluster capacity and usage
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph df

# Check storage classes and PVCs
oc get storageclass | grep ocs
oc get pvc -A | grep ocs

# Check node labels for ODF storage nodes
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"

# Check for ODF-related alerts
oc get alerts -n openshift-storage
oc get events -n openshift-storage --sort-by='.lastTimestamp' | grep -i "ceph\|odf\|storage"

# Check Ceph monitor status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon stat
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon dump

# Check CephFS status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph fs status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mds stat

# Check RGW (S3) status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph rgw stat
oc get pods -n openshift-storage | grep rgw

oc patch storagecluster ocs-storagecluster -n openshift-storage --type json --patch '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

== Understanding OpenShift Data Foundation

=== Diagnostic Steps: ODF Architecture Overview

OpenShift Data Foundation provides software-defined storage using Ceph as the underlying storage system. Key components include:

* **Ceph Monitors (MONs)**: Maintain cluster maps and coordinate between components
* **Object Storage Daemons (OSDs)**: Store data and handle replication/recovery
* **Managers (MGRs)**: Track metrics and expose cluster information via REST API
* **Metadata Servers (MDSes)**: Store metadata for CephFS file system operations
* **RADOS Gateway (RGW)**: Provides S3-compatible object storage interface

=== Information Gathering: ODF Component Status

Check the overall status of ODF components:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf

# View storage cluster status
oc get storagecluster -n openshift-storage

# Check Ceph cluster health
oc get cephcluster -n openshift-storage

# View all ODF pods
oc get pods -n openshift-storage

# Check ODF dashboard status
oc get route -n openshift-storage
----

=== Key ODF Tools and Resources

* **Ceph Toolbox**: Interactive pod for Ceph CLI commands
* **Must-Gather**: Comprehensive diagnostic data collection
* **ODF Console**: Web-based monitoring and management
* **Storage Classes**: Defines storage types (block, file, object)
* **Persistent Volume Claims**: Application storage requests

== Exercise 1: Troubleshooting ODF Health Issues

=== Scenario Setup: Storage Node Configuration Problems

Navigate to the ODF troubleshooting lab and deploy the exercise scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the ODF troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/00-odf-health

# Execute the ODF configuration script
chmod +x odf-config.sh
./odf-config.sh

# Monitor ODF cluster status
oc get storagecluster -n openshift-storage -w
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the ODF cluster behavior and identify any issues.

**What to check:**

- What is the status of the ODF storage cluster?
- Are there any alerts or warnings in the ODF console?
- Which nodes are labeled for ODF storage?
- Are there any OSD pods failing or in error state?
- What does the Ceph cluster health show?

Use the troubleshooting commands provided earlier to investigate the cluster behavior systematically.

=== Understanding the Problem

If you discovered that the ODF storage cluster is degraded and OSD pods are failing, this indicates a node labeling issue that prevents ODF from identifying storage nodes correctly.

=== Creating the Solution

**YOUR MISSION**: Identify and resolve the node labeling issue to restore ODF functionality.

**Steps to complete:**

1. **Identify the root cause** of the ODF cluster degradation
2. **Locate the affected storage nodes**
3. **Restore the missing node labels**
4. **Verify ODF cluster recovery**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation:**

Execute the fix script to restore ODF node labeling:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
./solution/odf-fix.sh
----
====

=== Verification

Monitor the ODF recovery process:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Watch ODF storage cluster status during recovery
oc get storagecluster -n openshift-storage -w

# Monitor OSD pod restart
oc get pods -n openshift-storage -w

# Check when all OSD pods are running
oc get pods -n openshift-storage | grep osd

# Verify Ceph cluster health after recovery
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status

# Verify node labels are restored
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"
----

== Exercise 2: File Storage Troubleshooting

Navigate to the file storage troubleshooting exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/02-odf-file

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-file
oc get pvc -n odf-file

# Verify PVC binding and size
oc get pvc -n odf-file
oc describe pvc upload-data -n odf-file

# Check storage class
oc describe storageclass ocs-storagecluster-cephfs

# Test application functionality
oc get route image-uploader -n odf-file -o jsonpath='{.spec.host}'
----

**Expected Application Behavior:**

- The application is expected to function on the previous HTTP route with 3 pods in Running state.
- Once all pods are in Running state, you should be able to upload all the files found in the directory `/home/demo-user/troubleshooting-lab/Photos` through the web interface. If you encounter any errors during the upload process, check the pod logs to understand what's happening.
- Application namespace is odf-file


=== Investigation Challenge

**YOUR MISSION**: Investigate the file storage issues and identify the root causes preventing the application from working properly.

**What to check:**

- How many pods are actually running and why?
- What is the current size of the PVC?
- What is the current AccessModes of the PVC?
- What happens when you try to upload files through the web interface?
- What type of storage is being used and can it be expanded?
- Can you change the AccessMode of an existing PVC?
- How could you resolve both the size and access mode issues?

**Investigation Steps:**

1. **Check pod status**: Are all 3 pods running or are some stuck in ContainerCreating/Pending?
2. **Examine PVC configuration**: Look at the current size and access modes
3. **Test file upload**: Try uploading a photo through the web interface
4. **Check pod logs**: Look for specific error messages when uploads fail
5. **Investigate storage class**: Understand what type of storage is being used

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Recreating PVC with Correct Configuration**

Execute the fix script to recreate the PVC with proper size and access mode:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-pvc.sh
./solution/fix-pvc.sh
----

**During the initial deployment, you will encounter 2 problems:**

- **AccessMode Issue**: This will cause errors and leave pods in ContainerCreating or Pending state

   ```
   Warning  FailedAttachVolume  2m51s  attachdetach-controller  Multi-Attach error for volume "pvc-d1f6cf6e-ee78-44a8-92fd-49f361113b73" Volume is already used by pod(s) image-uploader-745bb8fb7b-cb872, image-uploader-745bb8fb7b-ftmp4
   ```

- **PVC Size Issue**: This will prevent file uploads with errors like:

   ```
   OSError: [Errno 122] Disk quota exceeded
   ```

**Expected Behavior After Fix:**
After resolving both issues, the application should successfully upload all photos from the `/home/demo-user/troubleshooting-lab/Photos` directory through the web interface, with all 3 pods running and sharing the same storage volume.


====

== Exercise 3: Block Storage Troubleshooting

Navigate to the storage access exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/03-odf-block

# Deploy application with intentional issue
oc apply -k .
----

=== Information Gathering: Storage Access Investigation

Analyze the storage deployment problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check pod status
oc get pods -l app=rbd-hello -n odf-block -o wide

# Examine PVC configuration
oc get pvc -n odf-block
oc describe pvc rbd-hello-pvc -n odf-block

# Check storage class information
oc get storageclass
oc describe storageclass ocs-storagecluster-ceph-rbd

# Investigate pod scheduling issues
oc describe pod -l app=rbd-hello -n odf-block | grep -A 10 Events
----

=== Investigation Challenge

Investigate the storage access mode issue:

1. **Why can't multiple pods attach to the same volume?**
2. **What storage class is being used and what are its capabilities?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Access Mode Issues**

Execute the fix script to resolve the access mode problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-replicas.sh
./solution/fix-replicas.sh
----
====

**Expected Application Behavior:**
The application should display a "Hello World" message accessible via HTTP route once properly configured.

**Problem Analysis:**
Initially, the deployment will have 3 pods trying to use a `ocs-storagecluster-ceph-rbd` storage class with ReadWriteOnce (RWO) access mode. This creates a multi-attach error because RWO volumes can only be attached to one pod at a time.

**Error Details:**
You will see pods stuck in different states due to the multi-attach issue:

```
oc get pod
NAME                         READY   STATUS     RESTARTS   AGE
rbd-hello-74f4cbcd4c-76p6p   0/1     Init:0/1   0          2m3s
rbd-hello-74f4cbcd4c-hbhwn   0/1     Init:0/1   0          2m3s
rbd-hello-74f4cbcd4c-t6js6   0/1     Running    0          2m3s
```

**Error Message:**
```
Warning  FailedAttachVolume  2m20s  attachdetach-controller  Multi-Attach error for volume "pvc-4bffb8ac-1ab2-4b97-b1fd-4fcadbd2a64d" Volume is already used by pod(s) rbd-hello-74f4cbcd4c-t6js6
```

**Root Cause:**
The application is configured with 3 replicas but uses a ReadWriteOnce (RWO) Persistent Volume Claim, which can only be attached to one pod at a time. This causes the multi-attach error and prevents multiple pods from running simultaneously.

**Solution:**
To resolve this issue, the deployment should be scaled down to 1 replica since RWO volumes cannot be shared across multiple pods. After scaling down, the application will successfully display the "Hello World" message via the HTTP route.

== Lab Summary

This comprehensive ODF troubleshooting lab covered critical storage scenarios:

* **Storage Health**: Diagnosed and resolved node labeling issues affecting cluster performance
* **File Storage**: Resolved capacity issues through PVC expansion and storage type investigation
* **Block Storage**: Fixed multi-attach problems by understanding access modes and replica scaling
* **Object Storage**: Debugged authentication and configuration mismatches in bucket provisioning

== Additional Resources

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18[OpenShift Data Foundation Documentation]
* link:https://docs.ceph.com/en/quincy/[Ceph Documentation]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[ODF Troubleshooting Guide]
* link:https://access.redhat.com/solutions/4502501[Red Hat Solutions for ODF Issues]
