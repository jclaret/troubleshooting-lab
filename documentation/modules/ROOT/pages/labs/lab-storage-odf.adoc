= Storage - OpenShift Data Foundation Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

== Lab Overview

This comprehensive lab covers troubleshooting OpenShift Data Foundation (ODF) across multiple scenarios including Ceph cluster health issues, capacity management, file storage problems, block storage misconfigurations, and S3 object storage connectivity issues. You'll learn to diagnose and resolve storage problems in OpenShift environments.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand ODF architecture and Ceph cluster components
* Use ODF debugging tools including Ceph toolbox and must-gather
* Diagnose and resolve Ceph cluster health issues
* Troubleshoot storage capacity alerts and read-only conditions
* Resolve file storage (CephFS) capacity and mounting issues
* Fix block storage (RBD) multi-attach and access mode problems
* Debug S3 object storage connectivity and authentication issues
* Apply systematic troubleshooting methodology for storage problems

=== Reference Documentation

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[Troubleshooting OpenShift Data Foundation]
* link:https://docs.ceph.com/en/quincy/rados/troubleshooting/[Ceph Troubleshooting Guide]
* link:https://role.rhu.redhat.com/rol-rhu/app/seminar/exps275-1[OpenShift Data Foundation Expert Session]

== ODF Troubleshooting Commands

When troubleshooting OpenShift Data Foundation issues, use these commands to investigate and resolve problems:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf
oc get storagecluster -n openshift-storage
oc get cephcluster -n openshift-storage

# View all ODF pods and their status
oc get pods -n openshift-storage -o wide

# Check ODF dashboard and routes
oc get route -n openshift-storage

# Check Ceph cluster health using toolbox
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph health detail

# Check OSD tree and status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd tree
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd stat

# View cluster capacity and usage
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph df

# Check storage classes and PVCs
oc get storageclass | grep ocs
oc get pvc -A | grep ocs

# Check node labels for ODF storage nodes
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"

# Check for ODF-related alerts
oc get alerts -n openshift-storage
oc get events -n openshift-storage --sort-by='.lastTimestamp' | grep -i "ceph\|odf\|storage"

# Check Ceph monitor status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon stat
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon dump

# Check CephFS status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph fs status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mds stat

# Check RGW (S3) status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph rgw stat
oc get pods -n openshift-storage | grep rgw

oc patch storagecluster ocs-storagecluster -n openshift-storage --type json --patch '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

== Understanding OpenShift Data Foundation

=== Diagnostic Steps: ODF Architecture Overview

OpenShift Data Foundation provides software-defined storage using Ceph as the underlying storage system. Key components include:

* **Ceph Monitors (MONs)**: Maintain cluster maps and coordinate between components
* **Object Storage Daemons (OSDs)**: Store data and handle replication/recovery
* **Managers (MGRs)**: Track metrics and expose cluster information via REST API
* **Metadata Servers (MDSes)**: Store metadata for CephFS file system operations
* **RADOS Gateway (RGW)**: Provides S3-compatible object storage interface

=== Information Gathering: ODF Component Status

Check the overall status of ODF components:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf

# View storage cluster status
oc get storagecluster -n openshift-storage

# Check Ceph cluster health
oc get cephcluster -n openshift-storage

# View all ODF pods
oc get pods -n openshift-storage

# Check ODF dashboard status
oc get route -n openshift-storage
----

=== Key ODF Tools and Resources

* **Ceph Toolbox**: Interactive pod for Ceph CLI commands
* **Must-Gather**: Comprehensive diagnostic data collection
* **ODF Console**: Web-based monitoring and management
* **Storage Classes**: Defines storage types (block, file, object)
* **Persistent Volume Claims**: Application storage requests

== Exercise 1: Troubleshooting ODF Health Issues

=== Scenario Setup: Storage Node Configuration Problems

Navigate to the ODF troubleshooting lab and deploy the exercise scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the ODF troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/00-odf-health

# Execute the ODF configuration script
chmod +x odf-config.sh
./odf-config.sh

# Monitor ODF cluster status
oc get storagecluster -n openshift-storage -w
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the ODF cluster behavior and identify any issues.

**What to check:**

- What is the status of the ODF storage cluster?
- Are there any alerts or warnings in the ODF console?
- Which nodes are labeled for ODF storage?
- Are there any OSD pods failing or in error state?
- What does the Ceph cluster health show?

Use the troubleshooting commands provided earlier to investigate the cluster behavior systematically.

=== Understanding the Problem

If you discovered that the ODF storage cluster is degraded and OSD pods are failing, this indicates a node labeling issue that prevents ODF from identifying storage nodes correctly.

=== Creating the Solution

**YOUR MISSION**: Identify and resolve the node labeling issue to restore ODF functionality.

**Steps to complete:**

1. **Identify the root cause** of the ODF cluster degradation
2. **Locate the affected storage nodes**
3. **Restore the missing node labels**
4. **Verify ODF cluster recovery**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation:**

Execute the fix script to restore ODF node labeling:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
./solution/odf-fix.sh
----
====

=== Verification

Monitor the ODF recovery process:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Watch ODF storage cluster status during recovery
oc get storagecluster -n openshift-storage -w

# Monitor OSD pod restart
oc get pods -n openshift-storage -w

# Check when all OSD pods are running
oc get pods -n openshift-storage | grep osd

# Verify Ceph cluster health after recovery
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status

# Verify node labels are restored
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"
----

== Exercise 2: File Storage Troubleshooting

=== Problem Definition: File Upload Volume Issues

Navigate to the file storage troubleshooting exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/02-odf-file

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-file
oc get pvc -n odf-file
----

=== Information Gathering: File Storage Application Analysis

Monitor the file storage application and test file uploads:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check application deployment status
oc get pods -n odf-file

# Verify PVC binding and size
oc get pvc -n odf-file
oc describe pvc upload-data -n odf-file

# Check storage class
oc describe storageclass ocs-storagecluster-cephfs

# Test application functionality
ROUTE=$(oc get route image-uploader -n odf-file -o jsonpath='{.spec.host}')
curl http://$ROUTE/

# Try to upload files through web interface
echo "Try uploading files via the web interface at: http://$ROUTE/"
echo "Note: You should encounter storage space issues when uploading files"

# Run the test script to check current status
chmod +x test-upload.sh
./test-upload.sh
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the file storage capacity issue and identify the root cause.

**What to check:**

- What is the current size of the PVC?
- What happens when you try to upload files through the web interface?
- Are there any error messages in the application logs?
- What type of storage is being used and can it be expanded?
- How could you resize the PVC to resolve the storage issue?

**Investigation questions:**

1. **What happens when a file storage volume reaches capacity?**
2. **What type of storage is being used and can it be expanded?**
3. **How could you resize the PVC to resolve the storage issue?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Expanding File Storage**

Apply the fix to expand the PVC and resolve capacity issues:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Apply the complete solution
oc apply -k solution/

# Alternative manual steps:
# 1. Expand the PVC
oc patch pvc upload-data -n odf-file --type=merge -p '{
  "spec": { "resources": { "requests": { "storage": "1Gi" } } }
}'

# 2. Monitor resize completion
oc describe pvc upload-data -n odf-file | grep -i resize

# 3. Verify application functionality
oc rsh -n odf-file $POD df -h /data

# 4. Run fix script
chmod +x solution/fix-pvc.sh
./solution/fix-pvc.sh
----
====

== Exercise 3: Block Storage (RBD) Troubleshooting

=== Problem Definition: Multi-Attach Issues

Navigate to the block storage exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/00-storage-odf/03-odf-block

# Deploy block storage application with intentional issue
oc apply -f app-block.yaml
----

=== Information Gathering: RBD Multi-Attach Investigation

Analyze the block storage deployment problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check pod status
oc get pods -l app=rbd-hello -n odf-block -o wide

# Examine PVC configuration
oc get pvc -n odf-block
oc describe pvc rbd-hello-pvc -n odf-block

# Investigate pod scheduling issues
oc describe pod -l app=rbd-hello -n odf-block | grep -A 10 Events
----

Expected issue: Multiple pods trying to attach the same RWO (ReadWriteOnce) volume.

=== Investigation Challenge

Investigate the block storage access mode issue:

1. **Why can't multiple pods attach to the same RBD volume?**
2. **What are the different access modes for persistent volumes?**
3. **When would you use RWO vs RWX vs ROX?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Access Mode Issues**

Resolve the multi-attach problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Scale deployment to single replica
oc scale deployment/rbd-hello --replicas=1 -n odf-block

# Verify single pod is running
oc get pods -l app=rbd-hello -n odf-block

# Test application functionality
ROUTE=$(oc get route rbd-hello -n odf-block -o jsonpath='{.spec.host}')
curl http://$ROUTE/
----
====

== Exercise 4: Object Storage Troubleshooting

=== Problem Definition: Object Storage Authentication Issues

Navigate to the object storage troubleshooting exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the object storage troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/01-odf-s3

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-s3
oc get obc -n odf-s3
----

=== Information Gathering: Object Storage Configuration Analysis

Investigate object storage connectivity and authentication:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ObjectBucketClaim status
oc get obc -n odf-s3

# Examine generated ConfigMap and Secret
oc get cm img-bucket-a -n odf-s3 -o yaml
oc get secret img-bucket-a -n odf-s3 -o yaml

# Check application logs for authentication errors
oc logs -l app=s3-uploader -n odf-s3

# Check current environment configuration
oc describe deployment s3-uploader -n odf-s3 | grep -A 20 Environment

# Test object storage connectivity
URL=$(oc get route s3-uploader -n odf-s3 -o jsonpath='{.spec.host}')
curl http://$URL/

# Run the test script to check current status
chmod +x test-s3.sh
./test-s3.sh
----

Expected error: Access Denied or authentication failures.

=== Investigation Challenge

**YOUR MISSION**: Investigate the object storage authentication issue and identify the root cause.

**What to check:**

- What is the status of the ObjectBucketClaims?
- Are there any authentication errors in the application logs?
- Do the bucket names match the credentials being used?
- What is the relationship between ObjectBucketClaim, ConfigMap, and Secret?

**Investigation questions:**

1. **How does ODF provision object storage buckets and credentials?**
2. **What is the relationship between ObjectBucketClaim, ConfigMap, and Secret?**
3. **How do you troubleshoot object storage authentication issues?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Object Storage Configuration**

Apply the fix to correct the bucket and credential configuration:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Apply the complete solution
oc apply -k solution/

# Alternative manual steps:
# 1. Fix the ConfigMap reference in deployment
oc patch deployment s3-uploader -n odf-s3 --type=json -p='[
  {"op":"replace","path":"/spec/template/spec/containers/0/env/2/valueFrom/configMapKeyRef/name","value":"img-bucket-a"}
]'

# 2. Restart deployment
oc rollout restart deployment/s3-uploader -n odf-s3
oc rollout status deployment/s3-uploader -n odf-s3

# 3. Test object storage functionality
URL=$(oc get route s3-uploader -n odf-s3 -o jsonpath='{.spec.host}')
curl http://$URL/

# 4. Run fix script
chmod +x solution/fix-s3.sh
./solution/fix-s3.sh
----
====

== Lab Summary

This comprehensive ODF troubleshooting lab covered critical storage scenarios:

* **Storage Health**: Diagnosed and resolved node labeling issues affecting cluster performance
* **File Storage**: Resolved capacity issues through PVC expansion and storage type investigation
* **Block Storage**: Fixed multi-attach problems by understanding access modes and replica scaling
* **Object Storage**: Debugged authentication and configuration mismatches in bucket provisioning

== Additional Resources

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18[OpenShift Data Foundation Documentation]
* link:https://docs.ceph.com/en/quincy/[Ceph Documentation]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[ODF Troubleshooting Guide]
* link:https://access.redhat.com/solutions/4502501[Red Hat Solutions for ODF Issues]
