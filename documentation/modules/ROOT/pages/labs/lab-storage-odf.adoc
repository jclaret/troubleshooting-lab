= Storage - OpenShift Data Foundation Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

== Lab Overview

This comprehensive lab covers troubleshooting OpenShift Data Foundation (ODF) across multiple scenarios including Ceph cluster health issues, capacity management, file storage problems, block storage misconfigurations, and S3 object storage connectivity issues. You'll learn to diagnose and resolve storage problems in OpenShift environments.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand ODF architecture and Ceph cluster components
* Use ODF debugging tools including Ceph toolbox and must-gather
* Diagnose and resolve Ceph cluster health issues
* Troubleshoot storage capacity alerts and read-only conditions
* Resolve file storage (CephFS) capacity and mounting issues
* Fix block storage (RBD) multi-attach and access mode problems
* Debug S3 object storage connectivity and authentication issues
* Apply systematic troubleshooting methodology for storage problems

=== Reference Documentation

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[Troubleshooting OpenShift Data Foundation]
* link:https://docs.ceph.com/en/quincy/rados/troubleshooting/[Ceph Troubleshooting Guide]
* link:https://role.rhu.redhat.com/rol-rhu/app/seminar/exps275-1[OpenShift Data Foundation Expert Session]

== ODF Troubleshooting Commands

When troubleshooting OpenShift Data Foundation issues, use these commands to investigate and resolve problems:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf
oc get storagecluster -n openshift-storage
oc get cephcluster -n openshift-storage

# View all ODF pods and their status
oc get pods -n openshift-storage -o wide

# Check ODF dashboard and routes
oc get route -n openshift-storage

# Check Ceph cluster health using toolbox
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph health detail

# Check OSD tree and status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd tree
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph osd stat

# View cluster capacity and usage
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph df

# Check storage classes and PVCs
oc get storageclass | grep ocs
oc get pvc -A | grep ocs

# Check node labels for ODF storage nodes
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"

# Check for ODF-related alerts
oc get alerts -n openshift-storage
oc get events -n openshift-storage --sort-by='.lastTimestamp' | grep -i "ceph\|odf\|storage"

# Check Ceph monitor status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon stat
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mon dump

# Check CephFS status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph fs status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph mds stat

# Check RGW (S3) status
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph rgw stat
oc get pods -n openshift-storage | grep rgw

oc patch storagecluster ocs-storagecluster -n openshift-storage --type json --patch '[{ "op": "replace", "path": "/spec/enableCephTools", "value": true }]'
----

== Understanding OpenShift Data Foundation

=== Diagnostic Steps: ODF Architecture Overview

OpenShift Data Foundation provides software-defined storage using Ceph as the underlying storage system. Key components include:

* **Ceph Monitors (MONs)**: Maintain cluster maps and coordinate between components
* **Object Storage Daemons (OSDs)**: Store data and handle replication/recovery
* **Managers (MGRs)**: Track metrics and expose cluster information via REST API
* **Metadata Servers (MDSes)**: Store metadata for CephFS file system operations
* **RADOS Gateway (RGW)**: Provides S3-compatible object storage interface

=== Information Gathering: ODF Component Status

Check the overall status of ODF components:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ODF operator status
oc get csv -n openshift-storage | grep odf

# View storage cluster status
oc get storagecluster -n openshift-storage

# Check Ceph cluster health
oc get cephcluster -n openshift-storage

# View all ODF pods
oc get pods -n openshift-storage

# Check ODF dashboard status
oc get route -n openshift-storage
----

=== Key ODF Tools and Resources

* **Ceph Toolbox**: Interactive pod for Ceph CLI commands
* **Must-Gather**: Comprehensive diagnostic data collection
* **ODF Console**: Web-based monitoring and management
* **Storage Classes**: Defines storage types (block, file, object)
* **Persistent Volume Claims**: Application storage requests

== Exercise 1: Troubleshooting ODF Health Issues

=== Scenario Setup: Storage Node Configuration Problems

Navigate to the ODF troubleshooting lab and deploy the exercise scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the ODF troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/00-odf-health

# Execute the ODF configuration script
chmod +x odf-config.sh
./odf-config.sh

# Monitor ODF cluster status
oc get storagecluster -n openshift-storage -w
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the ODF cluster behavior and identify any issues.

**What to check:**

- What is the status of the ODF storage cluster?
- Are there any alerts or warnings in the ODF console?
- Which nodes are labeled for ODF storage?
- Are there any OSD pods failing or in error state?
- What does the Ceph cluster health show?

Use the troubleshooting commands provided earlier to investigate the cluster behavior systematically.

=== Understanding the Problem

If you discovered that the ODF storage cluster is degraded and OSD pods are failing, this indicates a node labeling issue that prevents ODF from identifying storage nodes correctly.

=== Creating the Solution

**YOUR MISSION**: Identify and resolve the node labeling issue to restore ODF functionality.

**Steps to complete:**

1. **Identify the root cause** of the ODF cluster degradation
2. **Locate the affected storage nodes**
3. **Restore the missing node labels**
4. **Verify ODF cluster recovery**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation:**

Execute the fix script to restore ODF node labeling:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
./solution/odf-fix.sh
----
====

=== Verification

Monitor the ODF recovery process:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Watch ODF storage cluster status during recovery
oc get storagecluster -n openshift-storage -w

# Monitor OSD pod restart
oc get pods -n openshift-storage -w

# Check when all OSD pods are running
oc get pods -n openshift-storage | grep osd

# Verify Ceph cluster health after recovery
oc -n openshift-storage rsh deployment/rook-ceph-tools ceph status

# Verify node labels are restored
oc get nodes -l cluster.ocs.openshift.io/openshift-storage -o custom-columns="NAME:.metadata.name,LABELS:.metadata.labels"
----

== Exercise 2: File Storage Troubleshooting

=== Problem Definition: File Upload Volume Issues

Navigate to the file storage troubleshooting exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/02-odf-file

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-file
oc get pvc -n odf-file

# Verify PVC binding and size
oc get pvc -n odf-file
oc describe pvc upload-data -n odf-file

# Check storage class
oc describe storageclass ocs-storagecluster-cephfs

# Test application functionality
oc get route image-uploader -n odf-file -o jsonpath='{.spec.host}'
----

The application is expected to function on the previous HTTP route with 3 pods running in Running state.

=== Investigation Challenge

**YOUR MISSION**: Investigate the file storage capacity issue and identify the root cause.

**What to check:**

- What is the current size of the PVC?
- What is the current AccessModes of the PVC?
- How many pods are actually running and why?
- What happens when you try to upload files through the web interface?
- What type of storage is being used and can it be expanded?

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Recreating PVC with Correct Configuration**

Execute the fix script to recreate the PVC with proper size and access mode:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-pvc.sh
./solution/fix-pvc.sh
----
====

== Exercise 3: Storage Access Issues

=== Problem Definition: Pod Scheduling Issues

Navigate to the storage access exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/03-odf-block

# Deploy application with intentional issue
oc apply -k .
----

=== Information Gathering: Storage Access Investigation

Analyze the storage deployment problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check pod status
oc get pods -l app=rbd-hello -n odf-block -o wide

# Examine PVC configuration
oc get pvc -n odf-block
oc describe pvc rbd-hello-pvc -n odf-block

# Check storage class information
oc get storageclass
oc describe storageclass ocs-storagecluster-ceph-rbd

# Investigate pod scheduling issues
oc describe pod -l app=rbd-hello -n odf-block | grep -A 10 Events
----

=== Investigation Challenge

Investigate the storage access mode issue:

1. **Why can't multiple pods attach to the same volume?**
2. **What are the different access modes for persistent volumes?**
3. **What storage class is being used and what are its capabilities?**
4. **When would you use different access modes?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Access Mode Issues**

Execute the fix script to resolve the access mode problem:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-replicas.sh
./solution/fix-replicas.sh
----
====

== Exercise 4: Storage Authentication Issues

=== Problem Definition: Application Authentication Problems

Navigate to the storage authentication exercise:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the storage authentication lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/04-storage-odf/01-odf-s3

# Deploy the exercise scenario
oc apply -k .

# Monitor application deployment
oc get pods -n odf-s3
oc get obc -n odf-s3
----

=== Information Gathering: Storage Configuration Analysis

Investigate storage connectivity and authentication:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check ObjectBucketClaim status
oc get obc -n odf-s3

# Examine generated ConfigMap and Secret
oc get cm img-bucket-a -n odf-s3 -o yaml
oc get secret img-bucket-a -n odf-s3 -o yaml

# Check application logs for authentication errors
oc logs -l app=s3-uploader -n odf-s3

# Check current environment configuration
oc describe deployment s3-uploader -n odf-s3 | grep -A 20 Environment

# Test application connectivity
URL=$(oc get route s3-uploader -n odf-s3 -o jsonpath='{.spec.host}')
curl http://$URL/

# Run the test script to check current status
chmod +x test-s3.sh
./test-s3.sh
----

=== Investigation Challenge

**YOUR MISSION**: Investigate the storage authentication issue and identify the root cause.

**What to check:**

- What is the status of the ObjectBucketClaims?
- Are there any authentication errors in the application logs?
- Do the bucket names match the credentials being used?
- What is the relationship between ObjectBucketClaim, ConfigMap, and Secret?

**Investigation questions:**

1. **How does the storage system provision buckets and credentials?**
2. **What is the relationship between ObjectBucketClaim, ConfigMap, and Secret?**
3. **How do you troubleshoot storage authentication issues?**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation: Fixing Storage Configuration**

Execute the fix script to correct the storage configuration:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Execute the fix script
chmod +x solution/fix-s3.sh
./solution/fix-s3.sh
----
====

== Lab Summary

This comprehensive ODF troubleshooting lab covered critical storage scenarios:

* **Storage Health**: Diagnosed and resolved node labeling issues affecting cluster performance
* **File Storage**: Resolved capacity issues through PVC expansion and storage type investigation
* **Block Storage**: Fixed multi-attach problems by understanding access modes and replica scaling
* **Object Storage**: Debugged authentication and configuration mismatches in bucket provisioning

== Additional Resources

* link:https://docs.redhat.com/en/documentation/red_hat_openshift_data_foundation/4.18[OpenShift Data Foundation Documentation]
* link:https://docs.ceph.com/en/quincy/[Ceph Documentation]
* link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.18/html/troubleshooting_openshift_data_foundation[ODF Troubleshooting Guide]
* link:https://access.redhat.com/solutions/4502501[Red Hat Solutions for ODF Issues]
