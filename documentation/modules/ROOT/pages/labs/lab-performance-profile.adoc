= Node Tuning Operator - Performance Profile Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

[#perfprofile]
== Lab Overview

This lab focuses on troubleshooting Performance Profile configurations in OpenShift, covering common issues encountered when deploying latency-sensitive workloads that require CPU isolation, huge pages, and kernel parameter tuning. You'll learn to diagnose and resolve Performance Profile misconfigurations that prevent applications from starting correctly.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand Performance Profile components and their purpose
* Diagnose Performance Profile application failures
* Resolve Machine Config Pool selector mismatches
* Troubleshoot kernel parameter tuning with custom Tuned profiles
* Debug huge pages allocation and availability issues

=== Reference Documentation

* link:https://github.com/openshift/cluster-node-tuning-operator/blob/main/docs/performanceprofile/troubleshooting.md[Performance Profile Troubleshooting Guide]
* link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile[CNF Tuning Low Latency Nodes with Performance Profile]
* link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/scalability_and_performance/cnf-tuning-low-latency-nodes-with-perf-profile#cnf-telco-ran-reference-design-performance-profile-template_cnf-low-latency-perf-profile[Telco RAN DU Reference Design Performance Profile]
* link:https://access.redhat.com/solutions/5532341[Performance Addons Operator Advanced Configuration]

== Understanding Performance Profiles

=== Performance Profile Components

Performance Profiles are a high-level abstraction that configures OpenShift nodes for latency-sensitive workloads. Key components include:

* **CPU Management**: Isolated vs reserved CPU cores for application and system workloads
* **Huge Pages**: Large memory pages to reduce TLB (Translation Lookaside Buffer) misses
* **Real-time Kernel**: Optional real-time kernel for deterministic latency
* **NUMA Topology**: Non-Uniform Memory Access configuration for memory locality
* **Kernel Parameters**: Low-level system tuning for performance optimization

=== Performance Profile Troubleshooting commands

After applying a Performance Profile, verify its status with the following commands:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check Performance Profile status
oc get performanceprofile
oc get performanceprofile -o yaml
oc get performanceprofiles.performance.openshift.io -o jsonpath='{.items[*].status.conditions[?(@.type=="Degraded")]}' | jq 'select(.status=="True")'

# Verify generated Tuned profiles
oc get profiles.tuned.openshift.io -A
oc get profiles.tuned.openshift.io -n openshift-cluster-node-tuning-operator

# Check Tuned CRDs and profiles
oc get tuned -n openshift-cluster-node-tuning-operator

# Check Node Tuning Operator pods
oc get pod -n openshift-cluster-node-tuning-operator
oc get pod -n openshift-cluster-node-tuning-operator -o wide

# Get Node Tuning Operator logs
NTO_POD=$(oc get pod -n openshift-cluster-node-tuning-operator -l name=cluster-node-tuning-operator -o jsonpath='{.items[0].metadata.name}')
oc logs -n openshift-cluster-node-tuning-operator $NTO_POD -f

# Check tuned daemon pods on specific nodes
oc -n openshift-cluster-node-tuning-operator get pods -l openshift-app=tuned -o wide

# Get nodes from workshop MCP
oc get nodes -l node-role.kubernetes.io/workshop -o name

# Get tuned pod running on each workshop node and check logs
for N in $(oc get nodes -l node-role.kubernetes.io/workshop -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}'); do
  echo "=== Node: $N ==="
  TUNED_POD=$(oc -n openshift-cluster-node-tuning-operator get pod -l openshift-app=tuned \
    --field-selector spec.nodeName=$N -o jsonpath='{.items[0].metadata.name}')
  echo "Tuned pod: $TUNED_POD"
  oc logs -n openshift-cluster-node-tuning-operator $TUNED_POD -c tuned --tail=50
done

# Check related MachineConfig and MachineConfigPool status
oc get mc,mcp,nodes -o wide

# Verify huge pages configuration on nodes
oc debug node/<node-name> -- chroot /host cat /proc/meminfo | grep -i huge
oc debug node/<node-name> -- chroot /host cat /sys/kernel/mm/transparent_hugepage/enabled
oc debug node/<node-name> -- chroot /host sysctl -n net.netfilter.nf_conntrack_max

# 
oc explain PerformanceProfile.spec.nodeSelector
----

== Exercise 1: Troubleshooting Huge Pages Application

=== Scenario Setup

You need to deploy an application that requires huge pages for optimal performance. The application has been configured to request huge pages resources, but there seems to be an issue with the deployment.

Navigate to the lab scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the performance profile lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/02-performance-profile/lab1-hugepages
----

=== Deploying the Application

Deploy the application and the Performance Profile configuration:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Deploy the huge pages application and configuration
oc apply -k .

# Check application status
oc get pods -n hugepages-web
----

=== Investigation Challenge

The application is not starting as expected. Use the troubleshooting commands provided at the beginning of this lab to investigate the issue.

**Your task**: Identify why the application is not running and resolve the problem.

**Time limit**: 15-20 minutes of investigation before checking the solution.

[TIP]
====
ðŸ’¡ **Investigation approach**: 

Start by examining the application status and then investigate the Performance Profile configuration. Use the commands from the "Performance Profile Troubleshooting commands" section above.

Key areas to investigate:

- Application pod status and events
- Performance Profile status and conditions
- Node labels and Machine Config Pool configuration
- Huge pages availability on nodes
====

=== Solution Implementation

.Click to reveal the solution
[%collapsible]
====
If you've completed your investigation, here's the solution:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Update the Performance Profile to use the workshop MCP
oc replace -k solution/

# Wait for Performance Profile to be applied
oc get performanceprofile lab1-hugepages -o yaml
oc get -n openshift-cluster-node-tuning-operator profiles.tuned.openshift.io

# Monitor MachineConfigPool rollout
oc get mcp workshop -w
----

In the deployed configuration we will find errors of the type 

      message: failed to find MCP with the node selector that matches labels "node.role.kubernetes.io/performance=fix-me"
      reason: GettingMachineConfigPoolFailed
      status: "True"
      type: Degraded

and 

      message: failed to find MCP with the node selector that matches labels "node.role.kubernetes.io/workshop="
      reason: GettingMachineConfigPoolFailed
      status: "True"
      type: Degraded

The node is expected to apply the Performance Profile configuration and restart the node belonging to the workshop MCP.
====

=== Verification

Verify that the huge pages application now starts successfully:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Verify application is now running
oc get pods -n hugepages-web
oc rollout status deployment/hugepages-web -n hugepages-web

# Access the application to verify huge pages configuration
oc get route hugepages-web -n hugepages-web -o jsonpath='{.spec.host}{"\n"}'
# curl http://<route-url> or open in browser

image::hugepages-app.png[]
----

== Exercise 2: Troubleshooting Kernel Parameter Tuning

=== Scenario Setup: Application Requiring Custom Kernel Parameters

You need to deploy an application that requires custom kernel parameters for optimal performance. The application has been configured with a Tuned profile, but there seems to be an issue with the configuration.

Navigate to the second lab scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the second performance profile lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/02-performance-profile/lab2-tuned
----

=== Deploying the Application

Deploy the application and its configuration:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Deploy the application and Tuned configuration
oc apply -k .

# Check application status
oc get pods -n conntrack-app
oc describe pod -n conntrack-app
oc logs deployment/conntrack-app -n conntrack-app
----

=== Investigation Challenge

**YOUR MISSION**: Investigate why the application is not starting correctly and resolve the configuration issues.

**Step-by-Step Investigation:**

- **What is the current status of the application pods?**
- **Are there any errors in the pod logs or events?**
- **What is the status of the Tuned CRD and profiles?**
- **Are there any errors in the Node Tuning Operator logs?**
- **What do the tuned daemon logs show on the nodes?**

Use the troubleshooting commands from the "Performance Profile Troubleshooting commands" section above to investigate the issue systematically. Focus on CRDs and operator logs.

=== Solution Implementation

.Click to reveal the solution
[%collapsible]
====
**Understanding the problem:**
- The Tuned profile has a wrong MCP configuration
- The Tuned profile has syntax errors in the kernel parameter configuration
- The `net.netfilter.nf_conntrack_max` parameter value is incorrectly formatted
- This prevents the Tuned profile from being applied correctly by the Node Tuning Operator

**Solution Implementation:**
Apply the corrected Tuned profile:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Remove the broken Tuned profile
oc delete -f tuned.yaml

# Apply the corrected Tuned profile
oc apply -k solution/

# Monitor Tuned CRD and profile application
oc get tuned -n openshift-cluster-node-tuning-operator
oc get profiles.tuned.openshift.io -n openshift-cluster-node-tuning-operator

# Check Node Tuning Operator logs for any errors
NTO_POD=$(oc get pod -n openshift-cluster-node-tuning-operator -l name=cluster-node-tuning-operator -o jsonpath='{.items[0].metadata.name}')
oc logs -n openshift-cluster-node-tuning-operator $NTO_POD --tail=20

# Check Node Tuning pod logs for any errors
oc get -n openshift-cluster-node-tuning-operator pods -o wide
TUNED_POD=$(oc get pod -n openshift-cluster-node-tuning-operator -l openshift-app=tuned -o jsonpath='{.items[0].metadata.name}')
oc logs -n openshift-cluster-node-tuning-operator $TUNED_POD

ERROR    tuned.utils.commands: Writing to file '/proc/sys/net/netfilter/nf_conntrack_max' error: '[Errno 22] Invalid argument'
----
====

=== Verification

Verify that the kernel parameter is correctly applied:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check updated net.netfilter.nf_conntrack_max value on a workshop node
NODE=$(oc get nodes -l node-role.kubernetes.io/workshop -o jsonpath='{.items[0].metadata.name}')
oc debug node/$NODE -- chroot /host sysctl net.netfilter.nf_conntrack_max

# Restart the application to pick up new kernel parameters
oc rollout restart deployment/conntrack-app -n conntrack-app
oc rollout status deployment/conntrack-app -n conntrack-app

# Verify application now starts successfully
oc get pods -n conntrack-app
oc logs deployment/conntrack-app -n conntrack-app

# Test application functionality
oc get route conntrack-app -n conntrack-app -o jsonpath='{.spec.host}{"\n"}'
# curl http://<route-url> should return success
----

== Lab Summary

This lab demonstrated critical troubleshooting skills for Performance Profile configurations:

* **MCP Selector Issues**: Resolved Performance Profile not applying due to missing Machine Config Pool
* **Node Labeling**: Correctly labeled nodes for custom Machine Config Pools
* **Tuned Profile Debugging**: Identified and fixed syntax errors in custom Tuned configurations
* **Kernel Parameter Tuning**: Applied custom kernel parameters through Tuned profiles
* **Application Dependencies**: Understood how applications depend on system-level performance configurations

== Additional Resources

* link:https://github.com/openshift/cluster-node-tuning-operator/blob/main/cmd/performance-profile-creator/README.md[Performance Profile Creator Documentation]
* link:https://github.com/openshift/cluster-node-tuning-operator/blob/main/examples/performanceprofile/samples/performance_v2_performanceprofile.yaml[Performance Profile Examples]
* link:https://docs.redhat.com/en/documentation/openshift_container_platform/4.19/html/scalability_and_performance/what-huge-pages-do-and-how-they-are-consumed[HugePages Documentation]
* link:https://access.redhat.com/solutions/7120569[OpenShift Node Conntrack Table Limitations]
