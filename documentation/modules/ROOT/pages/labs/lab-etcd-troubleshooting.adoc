= OpenShift Control Plane - ETCD Troubleshooting
include::../_attributes.adoc[]

:imagesdir: assets/images

== Lab Overview

This lab focuses on troubleshooting etcd cluster issues in OpenShift, covering scenarios where etcd components fail or become unavailable. You'll learn to diagnose etcd-related problems, understand the impact on cluster operations, and apply appropriate remediation techniques to restore etcd functionality.

=== Learning Objectives

By completing this lab, you will be able to:

* Understand etcd's role in OpenShift cluster operations
* Diagnose etcd cluster health and connectivity issues
* Identify etcd static pod failures and port conflicts
* Use etcd debugging tools and commands effectively
* Restore etcd functionality after service disruptions
* Analyze cluster operator degradation related to etcd issues

== etcd Troubleshooting Commands

When troubleshooting etcd cluster issues, use these commands to investigate and resolve problems:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check etcd cluster operator status
oc get clusteroperator etcd
oc describe clusteroperator etcd

# View etcd static pods
oc get pods -n openshift-etcd | grep etcd
oc get pods -n openshift-etcd -o wide

# Check etcd endpoints and member status
oc get endpoints -n openshift-etcd etcd
oc describe configmap etcd-endpoints -n openshift-etcd

# Check etcd member list and cluster health
NODE=$(oc get nodes -l node-role.kubernetes.io/master -o name | head -1 | cut -d'/' -f2)
oc rsh -n openshift-etcd etcd-$NODE etcdctl member list -w table
oc rsh -n openshift-etcd etcd-$NODE etcdctl endpoint health --cluster

# Check etcd operator logs
oc logs -n openshift-etcd-operator deployment/etcd-operator --tail=50

# Check etcd static pod logs
oc logs -n openshift-etcd etcd-$NODE --tail=50
oc logs -n openshift-etcd etcd-$NODE --previous || echo "No previous logs available"

# Monitor etcd cluster operator status
oc get co etcd -w
----

== Understanding etcd in OpenShift

Etcd is the key-value store that backs OpenShift's control plane, storing cluster configuration, API objects, cluster state, authentication data, and operational information. OpenShift deploys etcd as static pods on control plane nodes.

[IMPORTANT]
====
**etcd Quorum Requirements**:

- 3-node cluster: Tolerates 1 node failure (quorum = 2)
- Loss of quorum results in cluster read-only mode or complete failure
====

== Exercise 1: Understanding Normal etcd Operations

=== Information Gathering: Baseline etcd Health

Before introducing problems, establish baseline etcd health:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check overall cluster operator status
oc get co etcd

# Examine etcd operator detailed status
oc describe co etcd

# List all etcd-related pods
oc get pods -n openshift-etcd -o wide

# Check etcd member status
oc rsh -n openshift-etcd etcd-$(oc get nodes -l node-role.kubernetes.io/master -o name | head -1 | cut -d'/' -f2) etcdctl member list -w table

# Verify etcd connectivity
oc rsh -n openshift-etcd etcd-$(oc get nodes -l node-role.kubernetes.io/master -o name | head -1 | cut -d'/' -f2) etcdctl endpoint health
----

== Exercise 2: Troubleshooting etcd Cluster Issues

=== Scenario Setup

Navigate to the etcd troubleshooting lab directory and deploy the exercise scenario:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Change to the etcd troubleshooting lab directory
cd /home/demo-user/troubleshooting-lab/lab-materials/06-etcd

# Deploy the exercise scenario
./setup.sh

# Wait 5min until etcd got DEGRADED
# Monitor cluster operator status
watch "oc get co etcd; oc get pod -n openshift-etcd"
----

[WARNING]
====
**Lab Safety Notice**: This lab deliberately creates a critical cluster issue. Only perform this in a dedicated lab environment, never in production or shared development clusters.
====

=== Investigation Challenge

**YOUR MISSION**: Investigate the cluster behavior and identify any issues.

**What to check:**

- What is the status of the etcd cluster operator?
- Are there any etcd pods failing or in error state?
- Which control plane nodes are affected?
- What pods are running on the control plane nodes?
- Are there any port conflicts or network issues?

Use the troubleshooting commands provided earlier to investigate the cluster behavior systematically.

== Exercise 3: Resolving etcd Cluster Issues

=== Understanding the Problem

If you discovered that the etcd cluster operator is degraded and etcd pods are failing, this indicates a critical control plane issue that needs immediate attention.

=== Creating the Solution

**YOUR MISSION**: Identify and resolve the etcd cluster issue to restore cluster functionality.

**Steps to complete:**

1. **Identify the root cause** of the etcd failure
2. **Locate the affected control plane node**
3. **Remove the source of the disruption**
4. **Restart etcd services** to restore functionality
5. **Verify cluster recovery**

.Click to reveal the solution
[%collapsible]
====
**Solution Implementation:**

Apply the fix to restore etcd functionality:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Check etcd pod which is failing
oc logs etcd-ip-10-0-26-124.ec2.internal
...
Waiting for ports 2379, 2380 and 9978 to be released......

oc debug node/ip-10-0-26-124.ec2.internal -- chroot /host ss -tlpn sport = :2379

# Execute the fix script
oc apply -k solution/

# Alternative manual steps:
# 1. Delete the disruptive pod
oc delete pod break-etcd -n default

# 2. If etcd static pods are stuck, force delete them
AFFECTED_NODE=$(oc get nodes -l node-role.kubernetes.io/master -o name | head -1 | cut -d'/' -f2)
oc delete pod -n openshift-etcd etcd-$AFFECTED_NODE --force --grace-period=0

# 3. If etcd-guard pod exists and is problematic, remove it
oc delete pod -n openshift-etcd etcd-guard-$AFFECTED_NODE --force --grace-period=0 || echo "No etcd-guard pod to delete"
----
====

=== Verification

Monitor the etcd recovery process:

[.lines_space]
[.console-input]
[source,bash, subs="+macros,+attributes"]
----
# Watch etcd cluster operator status during recovery
watch oc get co etcd

# Monitor etcd pod restart
oc get pods -n openshift-etcd -w

# Check when all etcd pods are running
oc get pods -n openshift-etcd | grep etcd-

# Verify etcd cluster health after recovery
sleep 60  # Wait for pods to stabilize
NODE=$(oc get nodes -l node-role.kubernetes.io/master -o name | head -1 | cut -d'/' -f2)
oc rsh -n openshift-etcd etcd-$NODE etcdctl endpoint health --cluster

# Verify etcd member list
oc rsh -n openshift-etcd etcd-$NODE etcdctl member list -w table
----

== Lab Summary

This lab demonstrated critical etcd troubleshooting skills for OpenShift control plane management:

* **Service Disruption**: Simulated realistic etcd service failure through port conflicts
* **Impact Analysis**: Observed how etcd issues affect cluster operator status and functionality
* **Systematic Diagnosis**: Applied structured troubleshooting methodology to identify root causes
* **Recovery Procedures**: Restored etcd functionality through proper cleanup and restart procedures
* **Health Monitoring**: Implemented ongoing monitoring techniques for etcd cluster health

== Additional Resources

* link:https://access.redhat.com/articles/6967785[Consolidated Article for etcd guidelines with OpenShift Container Platform 4]
* link:https://access.redhat.com/articles/6271341[ETCD performance troubleshooting guide for OpenShift Container Platform]
